{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from planetaryimage import PDS3Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PUT THE DIRECTORY NAME OF THE NORMAL PICTURES IN imgDirNorm AND THE ANOMALOUS PICTURE IN imgDirAnom\n",
    "#THIS STEP MUST BE DONE BY HAND AS IDENTIFYING THE IMAGES FOR THE DATASET REQUIRES A HUMAN TOUCH\n",
    "#HOWEVER IF YOU DO NOT ALREADY HAVE A FOLDER WITH THE CURATED IMAGES HERES A LINK TO SAID IMAGES\n",
    "#FROM MY GITHUB THEY CAN BE UNPACKED VIA UNRAR AND USED FOR TESTING THE PIPELINE\n",
    "#https://github.com/AbhigyanKarmakar/GSOCAssessment\n",
    "\n",
    "#NOTE : THE DOWNLOAD STEP TAKES A LONG TIME BOTH VIA PYTHON REQUESTS AND WGET \n",
    "#SINCE ITS BEING DONE VIA A SINGLE THREAD, WHY IS IT A SINGLE THREAD YOU MAY ASK\n",
    "#WHENEVER I TRIED MULTITHREADED VERSIONS OF THE DOWNLOAD IT WOULD KEEP TEMPORARILY BANNING MY IP  :(\n",
    "#HOWEVER, IF YOU DO WANT TO TEST IT I AM ATTACHING THE SCRIPT FILES TO DO A MULTITHREADED DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''%%bash\n",
    "N=8\n",
    "(\n",
    "for year in $(seq 2011 2015); do\n",
    "\t(\n",
    "\tfor day in $(seq -w 1 365); do \n",
    "\t   ((i=i%N)); ((i++==0)) && wait\n",
    "\t      wget -nc -r -np -nH --cut-dirs=3 -R index.html -R index.html.tmp `echo \"https://pdsimage2.wr.usgs.gov/archive/mess-e_v_h-mdis-2-edr-rawdata-v1.0/MSGRMDS_1001/DATA/\"\"$year\"\"_\"\"$day/\"` & \n",
    "\t   sleep 2\n",
    "\tdone\n",
    "\t)\n",
    "done\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The images in the folder need to be in png/jpeg/jpg or in a similar format\n",
    "#Also make sure the directory name ends with '/' as shown in the example\n",
    "#If they aren't run the cell below\n",
    "imgDirNorm = '/media/abhigyank/Linux Mint 20_1 Cinnamon 64-bit/Norm1/'\n",
    "imgDirAnom = '/media/abhigyank/Linux Mint 20_1 Cinnamon 64-bit/Anom1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename),0)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT .IMG to .PNG\n",
    "#IF THE DATA IN THE FOLDERS imgDirNorm imgDirAnom is in .IMG format then run the script in the\n",
    "#2 cells below to convert it to .png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imgDirNorm\"\n",
    "python3 Saver.py \"$1\" \"$1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$imgDirAnom\"\n",
    "python3 Saver.py \"$1\" \"$1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imrotate(img , angle , scale=1):\n",
    "    (h, w) = img.shape\n",
    "    center = (w / 2, h / 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    return cv2.warpAffine(img, M, (h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA AUGMENTATION  (OPTIONAL)\n",
    "def augment_folder(folder):\n",
    "    augmented_images = []\n",
    "    images = load_images_from_folder(folder)\n",
    "\n",
    "    for i in images:\n",
    "        augmented_images.append(cv2.flip(i, 1))\n",
    "        augmented_images.append(cv2.flip(i, 0))\n",
    "        augmented_images.append(cv2.flip(i, -1))\n",
    "        rotate90 = imrotate(i,90)\n",
    "        rotate180 = imrotate(i,180)\n",
    "        rotate270 = imrotate(i,270)\n",
    "        augmented_images.append(cv2.flip(rotate90, 1))\n",
    "        augmented_images.append(cv2.flip(rotate180, 0))\n",
    "        augmented_images.append(cv2.flip(rotate270, -1))\n",
    "        augmented_images.append(rotate90)\n",
    "        augmented_images.append(rotate180)\n",
    "        augmented_images.append(rotate270)\n",
    "    \n",
    "    return augmented_images    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LOAD FROM DISK...........')\n",
    "\n",
    "Anomaly = load_images_from_folder(imgDirAnom)\n",
    "Normal = load_images_from_folder(imgDirNorm)\n",
    "\n",
    "print('DISK LOAD COMPLETE!')\n",
    "\n",
    "print('NORMAL IMAGES : ' , len(Normal))\n",
    "print('ANOMALOUS IMAGES : ' , len(Anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IMAGE AUGMENT')\n",
    "augmented_images = augment_folder(imgDirAnom)\n",
    "Anomaly = Anomaly + augmented_images\n",
    "print('IMAGE AUGMENT COMPLETE')\n",
    "\n",
    "print('NORMAL IMAGES : ' , len(Normal))\n",
    "print('ANOMALOUS IMAGES : ' , len(Anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATA SHUFFLE')\n",
    "random.shuffle(Anomaly)\n",
    "random.shuffle(Normal)\n",
    "random.shuffle(Anomaly)\n",
    "random.shuffle(Normal)\n",
    "print('SHUFFLE COMPLETE')\n",
    "\n",
    "print('NORMAL IMAGES : ' , len(Normal))\n",
    "print('ANOMALOUS IMAGES : ' , len(Anomaly))\n",
    "\n",
    "train_images=[]\n",
    "train_labels=[]\n",
    "test_images=[] \n",
    "test_labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LABLE CREATION')\n",
    "\n",
    "ratio = len(Normal)//len(Anomaly)\n",
    "\n",
    "i=0\n",
    "j=0\n",
    "\n",
    "while(i<len(Normal) and j<len(Anomaly)):\n",
    "    \n",
    "    if i%ratio:\n",
    "        if (Normal[i].shape[0]==512):\n",
    "            Normal[i] = cv2.resize(Normal[i]  , (1024 , 1024))  \n",
    "        train_images.append(np.reshape(Normal[i],(1024,1024,1)))\n",
    "        train_labels.append([0])\n",
    "\n",
    "    if not i%ratio:\n",
    "        if (Anomaly[j].shape[0]==512):\n",
    "            Anomaly[j] = cv2.resize(Anomaly[j]  , (1024 , 1024))            \n",
    "        train_images.append(np.reshape(Anomaly[j],(1024 , 1024 , 1)))\n",
    "        train_labels.append([1])\n",
    "        j+=1\n",
    "        if (Normal[i].shape[0]==512):\n",
    "            Normal[i] = cv2.resize(Normal[i]  , (1024 , 1024)) \n",
    "        train_images.append(np.reshape(Normal[i],(1024 , 1024 , 1)))\n",
    "        train_labels.append([0])\n",
    "        \n",
    "\n",
    "    i+=1\n",
    "print('LABELING COMPLETE')\n",
    "\n",
    "class_names = ['Clean', 'Anomaly Present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=np.array(train_images)/255.0\n",
    "train_labels=np.array(train_labels)/1.0\n",
    "test_images=np.array(test_images)/255.0 \n",
    "test_labels=np.array(test_labels)/1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintestratio = 0.70\n",
    "\n",
    "cutindex = int(traintestratio*len(train_images))\n",
    "\n",
    "test_images=train_images[cutindex:len(train_images)]\n",
    "train_images=train_images[0:cutindex]\n",
    "\n",
    "test_labels=train_labels[cutindex:len(train_labels)]\n",
    "train_labels=train_labels[0:int(traintestratio*len(train_images))]\n",
    "\n",
    "#print((train_images[0]).shape)\n",
    "#print(type(train_images))\n",
    "print(np.array(train_images).shape)\n",
    "print(np.array(train_labels).shape)\n",
    "print(len(test_images))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 1023, 1023, 64)    320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 511, 511, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 511, 511, 64)      4160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 255, 255, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 255, 255, 16)      1040      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 127, 127, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 127, 127, 16)      272       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 258064)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                4129040   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 4,134,866\n",
      "Trainable params: 4,134,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputshape = (1024, 1024, 1)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (2, 2), activation='relu', input_shape=inputshape))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (1, 1), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (1, 1), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (1, 1), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "model.save(\"./savedmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##INFERENCE\n",
    "savedModelDir = 'E:/GSOC FINAL SUBMISSION/savedmodel/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Unclean  || Actual  Anomaly Present\n",
      "Predicted Clean  || Actual  Anomaly Present\n",
      "Predicted Clean  || Actual  Clean\n",
      "Predicted Clean  || Actual  Anomaly Present\n"
     ]
    }
   ],
   "source": [
    "model = models.load_model(savedModelDir)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "inferData = load_images_from_folder(\"E:/InferImage/\")\n",
    "i=0\n",
    "while(i<len(inferData)):\n",
    "    if (inferData[i].shape[0]==512):\n",
    "        inferData[i] = cv2.resize(inferData[i]  , (1024 , 1024))  \n",
    "    i+=1\n",
    "actuallabel = [1,1,0,1]    #ENTER SEQUENCE OF ACTUAL CORRECT LABEL FOR INFERENCE FILE HERE \n",
    "\n",
    "for i in range (len(inferData)):\n",
    "    inferData[i] = np.reshape(inferData[i], (1024,1024,1))\n",
    "inferData = np.array(inferData)\n",
    "\n",
    "class_names = ['Clean', 'Anomaly Present']\n",
    "j=0\n",
    "for i in model.predict_classes(inferData):\n",
    "    actual = class_names[actuallabel[j]]\n",
    "    j+=1\n",
    "    if i ==0:\n",
    "        print('Predicted Clean' ,' || Actual ' , actual)\n",
    "    if i ==1:\n",
    "        print('Predicted Unclean',' || Actual ' , actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
